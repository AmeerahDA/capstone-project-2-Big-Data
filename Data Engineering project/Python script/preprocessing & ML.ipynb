{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have Try to download from Azure databricks Ipython Notebook but based on the size of the file i rewrite in VS with explaining everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toml \n",
    "- here we use toml that to read configuration data from a TOML file. and we can use a different way in azure or also we can use dotenv library that will read from .env files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "with open (\"/Workspace/Users/xavei.15-6@hotmail.com/info.toml\",\"r\") as f:\n",
    "    config_data = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mounting and unmounting \n",
    "- First we unmount the storage locatio before that to avoid a error and it's not necessary to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/sof-mountpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why of mounting Azure Blob storage \n",
    "- mounting an Azure Blob Storage container on Databricks File System (DBFS). For : \\\n",
    "       - storageAccountName \\\n",
    "       - containerName \\\n",
    "       - applicationId \\\n",
    "       - directoryID \\\n",
    "       - secretValue \n",
    "- we extract information from the `config_data` dictionary. This dictionary likely comes from a configuration file that stores sensitive details securely.\n",
    "- For `endpoint` it's acctuly mean that we constructs the endpoint URL for OAuth token acquisition using the directoryID. This URL points to the AAD service to get authorization tokens.\n",
    "- for `Source` we want to mount the Blob Storage contianer So we use the URI with format `abfss://` File system and that Follow the contianer name and storage account and end with the domain. \n",
    "- for configuration we use the key set for authentication type which we use `OAuth` for authorization , then we specified the key `OAuth` provider which is : `org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider` , then we set the application id and client secret and token endpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageAccountName = config_data['storageAccountName']\n",
    "containerName = config_data['containerName']\n",
    "applicationId = config_data['applicationId']\n",
    "directoryID = config_data['directoryID']\n",
    "secretValue = config_data['secretValue']\n",
    "endpoint = 'https://login.microsoftonline.com/' + directoryID + '/oauth2/token'\n",
    "source = 'abfss://' + containerName + '@' + storageAccountName + '.dfs.core.windows.net/'\n",
    "mountPoint = \"/mnt/sof-mountpoint\"\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": applicationId,\n",
    "           \"fs.azure.account.oauth2.client.secret\": secretValue,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": endpoint}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(source = source,mount_point = mountPoint, extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls('/mnt/sof-mountpoint')) # /mnt/sof-mountpoint -> it's will show us the two files that we have it in our Storage account and inside the contianer that we have, we have two folders one for daily and weekly loaded files and archive old file .\n",
    "\n",
    "\n",
    "display(dbutils.fs.ls('/mnt/sof-mountpoint/Landing'))# if we want to see what inside Landing folder we can add it into the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a spark session\n",
    "is we use the databricks inside azure here we don't need to create a session or sparkcontext, but if you work localy you need to createa a session before you start.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Table Loading\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing the Mounting Location Storage\n",
    "- By using the utilities function that we can review the blob storage and we direct it into `Sample-data-training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('/mnt/sof-mountpoint/Landing/sample-data-training/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to create a dataframe for Post and to doing that we need to use file location which mean that we recall the mount point of storage workshop that data mount in /mnt/sof-mountpoint/Landing\n",
    "file_location = '/mnt/sof-mountpoint/Landing/sample-data-training/post/*'# * here mean all file inside the path \n",
    "\n",
    "posts = spark.read.parquet(file_location) # here we creating the DataFrame be read the file_location as parquet which the files format inside the mount point \n",
    "num_columns = len(posts.schema.fields) # info about the num of columns that we have \n",
    "print(f'the number of columns that we have :{num_columns}') # 17\n",
    "\n",
    "display(posts) # 2,066 rows | 9.21 seconds runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas and DataFrames\n",
    "- There are several benefit from creating DataFrame and Save into local file system:\n",
    "> - Side of Scalability : we can store and process massive datasets without worrying about infrastructure limitations. which databricks leverages the underlying Azure cloud storage, which offers vast scalability. \n",
    "> - Side of Cost-Effectiveness: provides flexible storage options that cater to different needs and budgets \n",
    "> - Side of Performance and Efficiency: is readily accessible for various analytics tasks. Leveraging distributed processing, Databricks can analyze large datasets efficiently.Optimized data formats like Parquet (used in your code) further enhance query performance by enabling columnar storage and data skipping. \n",
    "> - Side of version control and Collabration : when the data saved in Databricks workspaces is accessible to authorized users, facilitating collaborative data analysis and exploration.Databricks provides version control capabilities, allowing you to track changes, revert to previous versions, and ensure data integrity. \n",
    "> - Side of integration and security : Azure Databricks seamlessly integrates with other Azure services like Azure Machine Learning, Azure Data Factory, and Azure Synapse Analytics. This streamlines data pipelines and workflows. Databricks adheres to stringent security standards and offers various access control mechanisms to protect sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in PostType and user we need to define(Build) the structure of the dataframe\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pt_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(),True),\n",
    "    StructField(\"Type\",StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we creating the PostType dataframe by defining the File_location and then we create it \n",
    "pt_file_location = '/mnt/sof-mountpoint/Landing/sample-data-training/PostTypes.txt'\n",
    "\n",
    "post_type = spark.read.option('header',True).option('sep',',').schema(pt_schema).csv(pt_file_location)\n",
    "display(post_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in PostType and user we need to define(Build) the structure of the dataframe\n",
    "\n",
    "users_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"CreationDate\", DateType(), True),\n",
    "    StructField(\"DisplayName\", StringType(), True),\n",
    "    StructField(\"DownVotes\", IntegerType(), True),\n",
    "    StructField(\"EmailHash\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Reputation\", IntegerType(), True),\n",
    "    StructField(\"UpVotes\", IntegerType(), True),\n",
    "    StructField(\"Views\", IntegerType(), True),\n",
    "    StructField(\"WebsiteUrl\", StringType(), True),\n",
    "    StructField(\"AccountId\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we creating the user dataframe by defining the File_location and then we create it \n",
    "user_file_location = '/mnt/sof-mountpoint/Landing/sample-data-training/users.csv'\n",
    "user = spark.read.option('header',True).option('sep',',').schema(users_schema).csv(user_file_location)\n",
    "\n",
    "display(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframes for easy retrieval So we will save 3 tables to databricks local file system\n",
    "posts.write.parquet('/temp/projects/sof/post.parquet')\n",
    "post_type.write.parquet('/temp/projects/sof/post_type.parquet')\n",
    "user.write.parquet('/temp/projects/sof/user.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we didn't set the overwrite mode may will get the below error that mean we already have these tables \n",
    "> [PATH_ALREADY_EXISTS] Path dbfs:/temp/projects/sof/post.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04\n",
    "File ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the local file system\n",
    "display(dbutils.fs.ls(\"/temp/projects/sof/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join tables and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import the necessary library\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to read the tables\n",
    "posts= spark.read.parquet('/temp/projects/sof/post.parquet')\n",
    "post_type = spark.read.parquet('/temp/projects/sof/post_type.parquet')\n",
    "user = spark.read.parquet('/temp/projects/sof/user.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this moment, we only use Posts and posttypes to train the model. so let's join them with the posttype id sence that the Post have column called posttypeid where we can \n",
    "# can join the two table through this key\n",
    "join_p_pt = posts.join(post_type,posts.PostTypeId == post_type.id)\n",
    "display(join_p_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the data\n",
    "In the `posttypes` table, there is a column called `Type` which indicates if the posts is a question or an answer. We only need the `question` entires. For these `Question` rows, we will run machine learning model on the join the `Body` column of the `Posts` table. To tell what topic this post is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_join_p_pt = join_p_pt.filter(col(\"Type\") == \"Question\")\n",
    "display(filtering_join_p_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are examples of `BODY AND TAGS` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BODY:<p>I have an abstract class with a protected variable</p> <pre><code>abstract class Beverage { protected string description; } </code></pre> <p>I can't access it from a subclass. \n",
    "    # Intellisense doesn't show it accessible. Why is that so?</p> <pre><code>class Espresso:Beverage { //this.description ?? }\n",
    "    # </code></pre>\n",
    "\n",
    "# Tags: <c#><.net><abstract-class><protected>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting for Machine Learning:\n",
    "- Machine learning algorithms typically work best with numerical data. Text data like the content in your Body and Tags columns needs preprocessing to be suitable for training these models.Removing HTML Tags, and we can use regexp_replace function removes HTML tags, and for Tags columns we can use split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need now to Formatting the 'Body' and `Tag` columns for machine learning training\n",
    "df = (filtering_join_p_pt.withColumn('Body',\\\n",
    "       regexp_replace(filtering_join_p_pt.Body, r'<.*?>', ''))# Transforming HTML code to strings\n",
    "      \n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \")) # Making a list of the tags\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include questions\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing the tags as individual tags instead of an array\n",
    "# This is duplicating the posts for each possible tag\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we didn't set the overwrite mode may will get the below error that mean we already have these the Checkpoint \n",
    "> Path dbfs:/tmp/project/sof.df.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint \n",
    "- as we know ML and data processing tasks can take hours or even days, unexpected events like cluster crashes, network issues, or software errors can interrupt the process and lead to wasted compute resources and time and that lead to `fault tolerance`\n",
    "- during the computation the checkpoint like saving point if the cluster terminated then you can start the process from last save checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file as a checkpoint (in case the cluster gets terminated)\n",
    "\n",
    "df.write.parquet(\"/tmp/project/sof.df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.cache():\n",
    "> This method instructs Spark to cache the DataFrame df in memory across the cluster.Purpose of using `cache()` Caching improves the performance of subsequent operations on the same DataFrame because Spark doesn't need to re-read the data from the original source (e.g., disk storage) every time.\n",
    "- Benefits:\n",
    "  - Faster execution for repeated operations on the same DataFrame.\n",
    "  - Useful when working with large datasets that are accessed multiple times within your notebook.\n",
    "> so we cache the DataFrame df after applying transformations like explode. This ensures faster execution if you perform further operations on the same DataFrame within your notebook.\n",
    "### df.count():\n",
    "- we use the `count` After caching fro serveal purpose:\n",
    "  - Verifies if the data has been loaded successfully and provides the total number of rows after the explode operation.\n",
    "  - Since the DataFrame is cached, counting should be relatively fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning Preprocessing\n",
    "- pyspark.sql.functions.regexp_replace is used to process the text\n",
    "* by using the regexp_replace we will do:\n",
    "   - Remove URLs such as http://stackoverflow.com\n",
    "   - Remove special characters\n",
    "   - Substituting multiple spaces with single space\n",
    "   - Lowercase all text\n",
    "   - Trim the leading/trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning = df.withColumn('text',regexp_replace(\"text\",r\"http\\S+\",\" \")) \\\n",
    "            .withColumn('text',regexp_replace(\"text\",r\"[^a-zA-z]\",\" \")) \\\n",
    "            .withColumn('text',regexp_replace(\"text\",r\"\\s+\",\" \")) \\\n",
    "            .withColumn('text',lower('text')) \\\n",
    "            .withColumn('text',trim('text'))\n",
    "display(data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries\n",
    "from pyspark.ml.feature import Tokenizer \n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for train and test:\n",
    "- we calls randomSplit() function which give us a split into two new dataframe based on giving weights\n",
    "    * `[0.9,0.1]` which we set for training `%90` and for testing `%10`.\n",
    "    * to ensure that whenever we run this line of code and the data will spilt in same way we use `seed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "train, test = data_cleaning.randomSplit([0.9, 0.1], seed=20200819)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "- to handle converting textual data into a format suitable for machine learning models we will: \n",
    "    * takes raw text data as input and transforms it into a sequence of tokens, which are smaller meaningful units like words or sub-words.\n",
    "- and Tokenization will help to removing punctuation and stop words (common words like \"the\", \"a\", \"an\") that don't contribute much to the meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Transformer Tokenizer\n",
    "tokenizer_field = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer_field.transform(data_cleaning)\n",
    "display(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Removal\n",
    "sw_remover = StopWordsRemover(inputCol='tokens',outputCol='filtered_word')\n",
    "sw = sw_remover.transform(tokenized)\n",
    "display(sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count vectorization it's aims to convert textual data into a numerical representation suitable for machine learning algorithms, also create a vocabulary (dictionary) of all unique words encountered in the training data. This vocabulary defines the features for the resulting numerical representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer (TF - Term Frequency)\n",
    "count_vectorizer = CountVectorizer(vocabSize=2**16,inputCol='filtered_word',outputCol='vectorization_count')\n",
    "cv_model=count_vectorizer.fit(sw)\n",
    "cv_text = cv_model.transform(sw)\n",
    "display(cv_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as we know for each document (text sample) in your data, the count vectorizer counts the number of times each word from the vocabulary appears in that document. This represents the term frequency (TF) for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "idf = IDF(inputCol='vectorization_count', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_model = idf.fit(cv_text)\n",
    "text_idf = idf_model.transform(cv_text)\n",
    "\n",
    "display(text_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as we know also ML can't directly process text data like the tags in your text_idf DataFrame so They require numerical features for computations. Label encoding addresses this by assigning a unique integer value to each distinct category (tag) in the \"tags\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "le_model = label_encoder.fit(text_idf)\n",
    "final = le_model.transform(text_idf)\n",
    "\n",
    "display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "logistic_reg = LogisticRegression(maxIter=100)\n",
    "\n",
    "logistic_reg_model = logistic_reg.fit(final)\n",
    "\n",
    "predictions = logistic_reg_model.transform(final)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evalution\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer_field, sw_remover, count_vectorizer, idf, label_encoder, logistic_reg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming (predicting) using the pipeline\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model object to the /mnt/deBDProject directory. Yours name may be different.\n",
    "pipeline_model.save('/mnt/sof-mountpoint/model')\n",
    "\n",
    "# Save the the String Indexer to decode the encoding. We need it in the future Sentiment Analysis.\n",
    "le_model.save('/mnt/sof-mountpoint/stringindexer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the directory\n",
    "display(dbutils.fs.ls(\"/mnt/sof-mountpoint/model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
